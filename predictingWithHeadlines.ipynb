{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:49926</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>16.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:49926' processes=2 threads=4, memory=16.00 GB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import get\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from progressbar import ProgressBar\n",
    "#pbar = ProgressBar()\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import Sequential\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, progress\n",
    "import os\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='8GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Tickers\n",
    "\n",
    "So, for some reason, I can't find a nice downloadable csv file that already has all of the tickers in the nyse. I did, however, find a website that can be used to scrape off of!\n",
    "So first, lets get all the tickers we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"curl --ftp-ssl anonymous:jupi@jupi.com \"\n",
    "          \"ftp://ftp.nasdaqtrader.com/SymbolDirectory/nasdaqlisted.txt \"\n",
    "          \"> nasdaq.lst\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading from that file\n",
    "with open('nasdaq.lst', 'r') as f:\n",
    "    file = f.read()\n",
    "find_string = file.split('</html>')\n",
    "ticker_string = find_string[len(find_string)-1]\n",
    "rows = ticker_string.split('\\n')\n",
    "tickers = [row.split('|')[0] for row in rows][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----AACG----\n",
      "     timestamp    open    high    low   close  adjusted_close  volume  \\\n",
      "0   2020-09-10  1.0800  1.0900  1.070  1.0700          1.0700   26856   \n",
      "1   2020-09-09  1.1100  1.1400  1.070  1.0900          1.0900    9963   \n",
      "2   2020-09-08  1.0900  1.1400  1.070  1.1400          1.1400   17481   \n",
      "3   2020-09-04  1.0900  1.1100  1.068  1.1000          1.1000   27247   \n",
      "4   2020-09-03  1.1320  1.1500  1.060  1.1100          1.1100   32074   \n",
      "..         ...     ...     ...    ...     ...             ...     ...   \n",
      "95  2020-04-27  0.7500  0.7877  0.612  0.6730          0.6730   62808   \n",
      "96  2020-04-24  0.8102  0.8102  0.750  0.7500          0.7500    7119   \n",
      "97  2020-04-23  0.7768  0.7780  0.735  0.7400          0.7400    3908   \n",
      "98  2020-04-22  0.7700  0.7799  0.750  0.7640          0.7640   16506   \n",
      "99  2020-04-21  0.7800  0.7801  0.702  0.7702          0.7702   10304   \n",
      "\n",
      "    dividend_amount  split_coefficient  \n",
      "0               0.0                1.0  \n",
      "1               0.0                1.0  \n",
      "2               0.0                1.0  \n",
      "3               0.0                1.0  \n",
      "4               0.0                1.0  \n",
      "..              ...                ...  \n",
      "95              0.0                1.0  \n",
      "96              0.0                1.0  \n",
      "97              0.0                1.0  \n",
      "98              0.0                1.0  \n",
      "99              0.0                1.0  \n",
      "\n",
      "[100 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\VApps\\Anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "----AACQ----\n",
      "    timestamp   open    high    low  close  adjusted_close   volume  \\\n",
      "0  2020-09-10  10.00  10.025  9.836   9.96            9.96  1758528   \n",
      "1  2020-09-09  10.00  10.010  9.910   9.99            9.99  1775622   \n",
      "2  2020-09-08  10.27  10.270  9.915   9.95            9.95    82469   \n",
      "3  2020-09-04  10.00  10.000  9.950  10.00           10.00    11506   \n",
      "\n",
      "   dividend_amount  split_coefficient  \n",
      "0              0.0                1.0  \n",
      "1              0.0                1.0  \n",
      "2              0.0                1.0  \n",
      "3              0.0                1.0  \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-aae58c9bf0c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprice_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdaily\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprice_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mnews_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNews\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprice_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnews_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\delphi-master\\delphi-master\\get.py\u001b[0m in \u001b[0;36mNews\u001b[1;34m(ticker)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"https://www.wsj.com/market-data/quotes/{ticker}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'User-Agent'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mNews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ul'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"newsSummary_c\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mNews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mNews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mNews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mNews\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "data = dict()\n",
    "for t in tickers:\n",
    "   # try:\n",
    "    print(f'----{t}----')\n",
    "    price_data = get.daily(t)\n",
    "    print(price_data)\n",
    "    news_data = get.News(t)\n",
    "    data.update({t:[price_data, news_data]})\n",
    "    print('------------')\n",
    "   # except AttributeError:\n",
    "    #    pass\n",
    "#In case this expirement needs to be replicated\n",
    "# import pickle\n",
    "# with open('stockReg.pkl', 'wb') as f:\n",
    "#     pickle.dump(data, f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I haven't finished working on data processing/scraping for this one. Below is the idea I had for using ML to determine price direction from market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
